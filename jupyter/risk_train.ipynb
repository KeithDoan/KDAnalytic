{"nbformat_minor": 1, "cells": [{"source": "# Train Credit Risk Model", "cell_type": "markdown", "metadata": {}}, {"source": "## Load data", "cell_type": "markdown", "metadata": {}}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "## Create a model", "cell_type": "markdown", "metadata": {}}, {"source": "spark_df = df_data_1.drop(\"State\").drop(\"City\")\n(train_data, test_data) = spark_df.randomSplit([0.8, 0.2], 24)\n\nMODEL_NAME = \"Risk Model\"\nDEPLOYMENT_NAME = \"Risk Deployment\"\n\nprint(\"Number of records for training: \" + str(train_data.count()))\nprint(\"Number of records for evaluation: \" + str(test_data.count()))\n\n", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "## Define model features and Label", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model\n\nsi_CheckingStatus = StringIndexer(inputCol = 'CheckingStatus', outputCol = 'CheckingStatus_IX')\nsi_CreditHistory = StringIndexer(inputCol = 'CreditHistory', outputCol = 'CreditHistory_IX')\nsi_LoanPurpose = StringIndexer(inputCol = 'LoanPurpose', outputCol = 'LoanPurpose_IX')\nsi_ExistingSavings = StringIndexer(inputCol = 'ExistingSavings', outputCol = 'ExistingSavings_IX')\nsi_EmploymentDuration = StringIndexer(inputCol = 'EmploymentDuration', outputCol = 'EmploymentDuration_IX')\nsi_Sex = StringIndexer(inputCol = 'Sex', outputCol = 'Sex_IX')\nsi_OthersOnLoan = StringIndexer(inputCol = 'OthersOnLoan', outputCol = 'OthersOnLoan_IX')\nsi_OwnsProperty = StringIndexer(inputCol = 'OwnsProperty', outputCol = 'OwnsProperty_IX')\nsi_InstallmentPlans = StringIndexer(inputCol = 'InstallmentPlans', outputCol = 'InstallmentPlans_IX')\nsi_Housing = StringIndexer(inputCol = 'Housing', outputCol = 'Housing_IX')\nsi_Job = StringIndexer(inputCol = 'Job', outputCol = 'Job_IX')\nsi_Telephone = StringIndexer(inputCol = 'Telephone', outputCol = 'Telephone_IX')\nsi_ForeignWorker = StringIndexer(inputCol = 'ForeignWorker', outputCol = 'ForeignWorker_IX')", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "si_Label = StringIndexer(inputCol=\"Risk\", outputCol=\"label\").fit(spark_df)\nlabel_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_Label.labels)", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "va_features = VectorAssembler(inputCols=[\"CheckingStatus_IX\", \"CreditHistory_IX\", \"LoanPurpose_IX\", \"ExistingSavings_IX\", \"EmploymentDuration_IX\", \"Sex_IX\", \\\n                                         \"OthersOnLoan_IX\", \"OwnsProperty_IX\", \"InstallmentPlans_IX\", \"Housing_IX\", \"Job_IX\", \"Telephone_IX\", \"ForeignWorker_IX\", \\\n                                         \"LoanDuration\", \"LoanAmount\", \"InstallmentPercent\", \"CurrentResidenceDuration\", \"LoanDuration\", \"Age\", \"ExistingCreditsCount\", \\\n                                         \"Dependents\"], outputCol=\"features\")", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "## Build A Credit Risk Model using Random Forest ", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.ml.classification import RandomForestClassifier\nclassifier = RandomForestClassifier(featuresCol=\"features\")\n\npipeline = Pipeline(stages=[si_CheckingStatus, si_CreditHistory, si_EmploymentDuration, si_ExistingSavings, si_ForeignWorker, si_Housing, si_InstallmentPlans, si_Job, si_LoanPurpose, si_OthersOnLoan,\\\n                               si_OwnsProperty, si_Sex, si_Telephone, si_Label, va_features, classifier, label_converter])\nmodel = pipeline.fit(train_data)", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "## Model Evaluation", "cell_type": "markdown", "metadata": {}}, {"source": "predictions = model.transform(test_data)\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\narea_under_curve = evaluatorDT.evaluate(predictions)\n\n#default evaluation is areaUnderROC\nprint(\"areaUnderROC = %g\" % area_under_curve)", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "## Save and deploy the model", "cell_type": "markdown", "metadata": {}}, {"source": "WML_CREDENTIALS = {\n  \"apikey\": \"DLWWjSw5Vttta3k4kccmlPqAo2otR8-3FuQRLDDUiRWE\",\n  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:pm-20:us-south:a/6bf3c9070c42bf0f4ea682367d8f4f81:e2a27a67-8c2c-43d3-92b6-efcded30a28b::\",\n  \"iam_apikey_name\": \"auto-generated-apikey-a93f2ba3-750d-4b7b-97b7-a380d315cd1c\",\n  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/6bf3c9070c42bf0f4ea682367d8f4f81::serviceid:ServiceId-1e9dd134-cbc8-4c23-b606-d3330ab35c66\",\n  \"instance_id\": \"e2a27a67-8c2c-43d3-92b6-efcded30a28b\",\n  \"password\": \"d53b23fe-3f18-4dfe-bc36-da42654bf472\",\n  \"url\": \"https://us-south.ml.cloud.ibm.com\",\n  \"username\": \"a93f2ba3-750d-4b7b-97b7-a380d315cd1c\"\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient\nimport json\n\nwml_client = WatsonMachineLearningAPIClient(WML_CREDENTIALS)", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "### List existing model ", "cell_type": "markdown", "metadata": {}}, {"source": "model_deployment_ids = wml_client.deployments.get_uids()\nfor deployment_id in model_deployment_ids:\n    deployment = wml_client.deployments.get_details(deployment_id)\n    model_id = deployment['entity']['deployable_asset']['guid']\n    if deployment['entity']['name'] == DEPLOYMENT_NAME:\n        print('Deleting deployment id', deployment_id)\n        wml_client.deployments.delete(deployment_id)\n        print('Deleting model id', model_id)\n        wml_client.repository.delete(model_id)\nwml_client.repository.list_models()", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "### Define model properties: model name, evaluation method, metrics and threshold", "cell_type": "markdown", "metadata": {}}, {"source": "model_props = {\n    wml_client.repository.ModelMetaNames.NAME: \"{}\".format(MODEL_NAME),\n    wml_client.repository.ModelMetaNames.EVALUATION_METHOD: \"binary\",\n    wml_client.repository.ModelMetaNames.EVALUATION_METRICS: [\n        {\n           \"name\": \"areaUnderROC\",\n           \"value\": area_under_curve,\n           \"threshold\": 0.7\n        }\n    ]\n}", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "### Storing model into Watson Machine Learning Repository", "cell_type": "markdown", "metadata": {}}, {"source": "wml_models = wml_client.repository.get_details()\nmodel_uid = None\nfor model_in in wml_models['models']['resources']:\n    if MODEL_NAME == model_in['entity']['name']:\n        model_uid = model_in['metadata']['guid']\n        break\n\nif model_uid is None:\n    print(\"Storing model ...\")\n\n    published_model_details = wml_client.repository.store_model(model=model, meta_props=model_props, training_data=train_data, pipeline=pipeline)\n    model_uid = wml_client.repository.get_model_uid(published_model_details)\n    print(\"Done\")", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "### Deploying model into Watson Machine Learning", "cell_type": "markdown", "metadata": {}}, {"source": "wml_deployments = wml_client.deployments.get_details()\ndeployment_uid = None\nfor deployment in wml_deployments['resources']:\n    if DEPLOYMENT_NAME == deployment['entity']['name']:\n        deployment_uid = deployment['metadata']['guid']\n        break\n\nif deployment_uid is None:\n    print(\"Deploying model...\")\n\n    deployment = wml_client.deployments.create(artifact_uid=model_uid, name=DEPLOYMENT_NAME, asynchronous=False)\n    deployment_uid = wml_client.deployments.get_uid(deployment)\n    \nprint(\"Model id: {}\".format(model_uid))\nprint(\"Deployment id: {}\".format(deployment_uid))", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.6 with Spark", "name": "python36", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.8", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}